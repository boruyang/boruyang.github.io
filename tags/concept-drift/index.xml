<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>concept drift on Boru Yang - About Data Science</title>
    <link>https://boruyang.github.io/tags/concept-drift/</link>
    <description>Recent content in concept drift on Boru Yang - About Data Science</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-tw</language>
    <lastBuildDate>Sat, 08 Apr 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://boruyang.github.io/tags/concept-drift/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>資料的滄海桑田 (I) - 概念飄移的來源與類型</title>
      <link>https://boruyang.github.io/blogs/concept_drift_01/</link>
      <pubDate>Sat, 08 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://boruyang.github.io/blogs/concept_drift_01/</guid>
      <description>前言 多數 AI 開發專案都不可避免地遇到這樣的問題：機器學習模型進入到部署階段並且開始服務後，隨著時間推移，資料分布出現飄移的情況，導致模型準確度隨著時間下降，最終使得專案的預期目標無法達成。
舉一個錯誤偵測模型應用於製造業的例子：假設有個模型的輸入為產品的生產歷程，輸出為該產品是否有瑕疵，即使這個模型在驗證集和測試集上都已經取得了符合要求的表現，等到模型開始上線運作之後，仍有許多因素可能使得資料的分布改變，例如機台隨著時間的損耗或者製造的原物料品質浮動等等，而這些因素都有可能讓模型表現漸漸不如預期，導致這個錯誤偵測模型的應用難以長久落地。
這種情況一般稱作為概念飄移 (concept drift)，概念飄移描述資料的分布隨著時間變化，使得根據過去資料歸納出來的規則以及訓練出來的模型，難以適用在新搜集的資料，並且降低模型預測的準確率或是導致錯誤的決策。
概念飄移對於機器學習模型的開發和部署帶來了非常大的挑戰。從模型開發的角度來說，假設訓練資料就已經存在概念飄移的情況，如何設計能夠適應資料分布不斷改變的模型，讓模型在訓練資料上的表現可以免於概念飄移的負面影響，甚至讓模型上線後也能自動地適應不同的資料分布，至今仍是活躍的研究議題。
而從模型部署的角度來說，模型開發者必須在模型上線後持續地監控模型表現，然而如何判斷模型表現確實受到概念飄移的污染，以及如何進行模型重訓練以校正模型的準確度，這些議題在實務上也有許多不同的討論和做法。
要解決以上的種種問題，必須要掌握概念飄移的幾個基本來源和類型。本文將從概念飄移的來源以及類型兩個角度，說明概念飄移呈現的不同形式。
概念飄移的來源 本文以下使用二元分類作為例子進行說明，令 $X$ 為特徵的隨機變數，且 $y$ 為二元標籤的隨機變數，目標是要給定一組 $X$ 得到預測的 $y$。假設在 $t$ 時間下觀察到了特徵 $X$ 與標籤 $y$ 的聯合分布 $P_t(X,y)$，根據貝氏定理可以進一步得到 $P_t(X,y)=P_t(X)P_t(y|X)$。
你現在也許已經開始混亂了，原本好好地要討論概念飄移，怎麼突然冒出了聯合分布 $P_t(X,y)$，又突然要把聯合分布進行拆解？這邊可以回過頭想一想，概念飄移要討論是「資料隨著時間變化」這件事情，而在二元分類的情境下，所擁有的資料正是 $X$ 與 $y$，這兩者的共同分布（也就是聯合分布）就是 $P(X,y)$。為了探討這個聯合分布在時間上的變化，又多了一個在 $t$ 時間下的假設，才會是最一開始提到的 $P_t(X,y)$。
至於為何要將 $P_t(X,y)$ 進行拆解？是因為單從 $P_t(X,y)$ 這個式子本身會如何隨著時間變化難以看出端倪，拆解成 $P_t(X)$ 和 $P_t(y|X)$ 兩個部分後，才能進一步研究 $P_t(X,y)$ 變化背後的成因。$P_t(X)$ 可以理解為在 $t$ 時間下特徵資料 $X$ 的分布，而 $P_t(y|X)$ 可以理解為在 $t$ 時間下，給定了一組特徵之後二元標籤的分布。由於透過 $P_t(y|X)$ 可以觀察出不同的特徵對應到的二元標籤，故 $P_t(y|X)$ 也可以視為 $t$ 時間下二元標籤的決策界線 (decision boundary)。
因此根據以上的拆解，當概念飄移發生，也就是 $P_t(X,y)\neq P_{t+1}(X,y)$ 時，飄移可能的來源有以下三種。
$P_t(X)\neq P_{t+1}(X)$，代表在決策界線沒有改變的前提之下，特徵資料分布發生改變，這種飄移的特性在於只有模型輸入的分布改變，輸入和輸出間的關係則保持不變，因此對於模型表現的損害有限。又因決策界線沒有發生改變，這種來源的飄移又被稱作虛擬飄移 (virtual drift)，其他名稱包括資料飄移 (data drift) 以及共變量飄移 (covariate drift)。 下圖為虛擬飄移的示意圖，藍色的圓形和三角形代表 $t$ 時間之下的資料分布（特徵與二元標籤），藍色的線代表 $t$ 時間之下的決策界線，而紅色的點則代表 $t+1$ 時間下產生變化之後的結果。從圖中可以看到，只要決策界線維持不變，即便特徵的分布出現偏移，模型仍能維持相同的表現水準。</description>
    </item>
    
  </channel>
</rss>
